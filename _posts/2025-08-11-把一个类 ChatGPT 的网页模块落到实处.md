---
layout:     post
title:      "把一个类 ChatGPT 的网页模块落到实处"
subtitle:   ""
date:       2025-08-11
author:     "漆黑菌"
header-img: "img/post-bg.jpg"
catalog: true
tags:
    - ChatGPT
    - SSE
    - 流式渲染
---

### 把一个类 ChatGPT 的网页模块落到实处：工程实践与取舍

这次做的是一个“部分依赖 LLM 的类 ChatGPT 网页”。目标是在浏览器端把“流式文字输出”和“可中断、可恢复”的体验做扎实，同时准备好降级路径。同时在成本控制上做了些许优化。

### 消息模型：按组件拼接，并且可以阻塞等待
- 背景：在产品认知中，“像 ChatGPT 一样一段一段吐字，不要一次性出完”才是“类 ChatGPT”的正确打开方式。
- 做法：每一条消息都是一个组件实例（含 props 和 emits）或者 Render 函数。必要时返回一个可等待的 promise（比如打字机结束），流水线就能自然“等一等、再往下走”。
- 收益：把“节奏感”变成能力，而不是写死在某个组件里；后续插入/替换消息类型也简单。

### 流传输策略：SSE 优先，失败降级到轮询
- 选择：优先用“流式”传输（SSE），失败或环境不稳时自动降级到轮询。
- 触发点：SSE 重试次数超过阈值，或建立连接失败时，直接切换到轮询。
- 数据对齐：SSE 和轮询的字段尽量保持一致，任何时刻都能“断—>换接口—>继续传”。

### 为什么自实现“类 SSE”（基于 axios）而不是原生 EventSource
- 现状：项目里已有较重的 axios 基建（拦截器、鉴权、重试、baseURL、代理、超时、统一错误处理等），EventSource 难以复用这些能力。
- 现实诉求：
  - 需要用 POST 发起“流式解释”（EventSource 标准只支持 GET）。
  - 需要自定义 `Last-Event-ID` 的 header 名称（网关对白名单 header 有要求，不在白名单里配置较麻烦）。
- 取舍：在 axios 的 onDownloadProgress 上复刻 SSE 解析与重连，更容易“沿用已有基建”，减少一套并行体系的维护成本。
- 风险与对策：
  - 自己维护解析器/重连策略，需要单测（可参考sse.js的测试用例）。
  - 与原生 EventSource 语义差异要明示（readyState、错误事件），避免误用。
- 结论：统一到 axios 体系能降低后续维护成本，代价是我们要对“流式解析/重连/资源释放”负责到底。

说明：默认 `DefaultLastEventIdHeaderName` 设为通用的 `Last-Event-ID`，但实际发起会使用网关白名单允许的名称（例如 `xxxxx-last-event-id`）。

### 状态机设计（前端视角）
- 状态集合：`Ready → Pending → Loading → Success`；中断态：`Stop`；错误态：`Error`、`SystemError`。
- 迁移规则（简化）：
  - start/create → `Pending`。
  - 收到 `index = 0` → `Pending`；`index > 0` → `Loading`；`index = -1` → `Success`。
  - 用户手动终止 → `Stop`（UI 立即响应，再补发取消）。
  - 普通错误（网络抖动等）→ `Error`（可重试、可降级）。
  - 系统错误（任务失败/取消等不可恢复）→ `SystemError`（不再自动重试）。
- 设计意图：把“可重试的错误”和“重试无意义的错误”拆开，避免无意义消耗，且用户操作的中断语义独立于错误语义。

### 先用 prompt 换 token，再开始流式
- 原因：降级到轮询时，若直接带 prompt，可能一次要传二十多 KB，但服务端一跳只回几十字节，传输效率极差。
- 做法：先调用“prompt→token”接口，后续 SSE/轮询都只带 token + 少量上下文，带宽友好。

### 后端契约与边界
- 完成信号：采用 `index = -1` 表示“最后一跳已下发完成”。直接断开连接的话，前端无法区分“后端主动结束”还是“网络问题”。协议是：后端在下发 `index = -1` 的同时直接关闭；前端在收到后也会主动 close（但不保证一定传回后端，影响不大）。
- 事件类型：后端通过 `event` 区分不同事件类型（参考后端状态机文档），前端按类型决定渲染/状态迁移。
- 断点续传：要求后端支持 `lastEventId` 风格的断点续传。
- Redis 缓存时效：极端情况下，缓存 5 分钟还没传完会报错；可以接受，正常量级下 5 分钟足够。
- 命中缓存与请求约定：
  - 请求不带 `job_id`：视为“全新请求”，允许命中缓存（直出已缓存分段）。
  - 请求带 `job_id`：强制从 Redis 继续分段，确保序与幂等。
- 取消：前端需显式“断开 SSE 或停止轮询”，并用“取消参数”通知后端任务取消；对“跨用户可复用缓存”的接口，取消是“假取消”（不影响缓存回放）；其它任务则返回 `{ index: -1, job_status: 取消 }`。
- 前端本地存储：需保留每段的 `index`，重连/降级时可按序拼接与去重。

### 降级到轮询：为什么与怎么做
- 为什么：目标运行环境里缺乏对 SSE 的历史验证与压测，需准备保底方案。
- 如何降级：
  - 轮询接口字段与 SSE 尽量一致；是否命中缓存、断开/取消等语义保持相同，做到幂等。
  - 命中缓存时，第 n 跳返回后，下一跳的 `index` 仍是 n（对齐 SSE 事件序列）。
  - 第 n 跳获取成功后，约 2 秒再发起 n+1 请求（避免打爆后端与代理）。
  - 任意状态下可断开，再用另一条链路无缝续上。

### LLM 成本优化（当前做法）
- 结果缓存：与后端一起在“可跨用户复用、且不含敏感信息”的解释上做缓存命中（降低重复推理成本）。
- 取消释放：前端“手动终止”会通知后端释放资源，减少无意义的推理占用。
- 传输粒度：通过 token 化缩小请求体，SSE/轮询都以“分段文案”传输，避免一次性大 payload。

### 动图格式选择：WebP 与 APNG（以及为什么不用 GIF）
- 背景与选择：WebP（有损/无损、可动图）与 APNG（无损、支持 alpha）在同等视觉质量下体积显著小于 GIF。
- iOS 兼容要点：
  - 目前来看iOS 16之前的 iOS/Safari 对“动图 WebP”的支持/稳定性不一致，会出现丢失部分帧信息。APNG 在 iOS 上长期表现更稳。
  - 策略：优先 WebP，iOS 或不支持场景回退 APNG/PNG；关键位保留静态首帧兜底。
- 使用建议：
  - 控制帧率/尺寸，限制循环次数；为 `<img>` 设定固有尺寸避免布局抖动。

### Markdown 渲染：也许能做得更好？增量更新与 CJK 友好
- 现状：渲染较为粗糙，流式到达的新文案会触发较大的重排；在 CJK 场景下，强调语法与标点边界不友好（例如 `**文字。**` 在部分解析器里不会被视为强调）。
- 增量更新路线：
  - 为每段落/片段维护 token（或 `index`），只对“新增片段”做渲染，避免整段重排。
  - 流程：后端分段下发 `index` 与文本 → 前端缓存 `index -> text` → 节流（50–100ms）合并“新增 index” → 仅对新片段做 Markdown→HTML 解析并 append → 错误/重连按 `index` 去重与补齐。
  - 解析放到 Web Worker，主线程仅做 DOM 拼接，降低卡顿风险。
- CJK 友好（强调边界）改造：
  - 问题根因：多数解析器的“可夹持边界”规则以西文空格/标点为主，`**文字。**` 会因紧邻句号被判定为不闭合。
  - 方案 A（推荐，零侵入）：预处理文本，把“强调+标点”拆开，等价为 `**文字**。`。
    ```ts
    function normalizeCjkEmphasis(src: string) {
      // 把 **内容**紧跟CJK/西文标点 的写法改为 **内容** 标点
      return src.replace(/\*\*([^\*]+?)\*\*([，。！？；：,.!?;:])/g, '**$1**$2');
    }
    ```
  - 方案 B：在强调内外自动插入零宽空格，兼容性较好但要注意复制粘贴体验。

### 一点经验
- 流式优先但“随时可降级”，配合状态机把“终止/错误/成功”分清，体验和稳定性都更好。
- 让前后端协议天然支持“断点续传”和“无缝切链路”，能显著降低线上不确定性带来的复杂度。
- 能 token 化的就 token 化，既省带宽，也便于做缓存与复用。
